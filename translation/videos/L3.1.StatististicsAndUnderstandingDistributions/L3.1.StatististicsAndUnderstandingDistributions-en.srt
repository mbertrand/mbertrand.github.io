0
00:00:00,000 --> 00:00:05,053


1
00:00:05,053 --> 00:00:06,220
INSTRUCTOR: Hello, everyone.

2
00:00:06,220 --> 00:00:07,520
Welcome back.

3
00:00:07,520 --> 00:00:09,760
In today's lecture, we are going to talk

4
00:00:09,760 --> 00:00:14,120
about statistics for data exploration and analytics.

5
00:00:14,120 --> 00:00:16,880
Let's begin with a simple question.

6
00:00:16,880 --> 00:00:18,920
What is statistics?

7
00:00:18,920 --> 00:00:22,680
Formally, statistics is a branch of mathematics focused

8
00:00:22,680 --> 00:00:25,540
on collecting, analyzing, interpreting,

9
00:00:25,540 --> 00:00:29,920
and presenting data to extract insights and support

10
00:00:29,920 --> 00:00:31,720
better decisions.

11
00:00:31,720 --> 00:00:36,360
In this lecture, we'll focus on descriptive statistics, methods

12
00:00:36,360 --> 00:00:39,120
that help us summarize and visualize data

13
00:00:39,120 --> 00:00:42,480
to reveal patterns and trends.

14
00:00:42,480 --> 00:00:45,040
Let's start with an example.

15
00:00:45,040 --> 00:00:48,400
Suppose we want to study the heights of students

16
00:00:48,400 --> 00:00:51,560
enrolled at Universal AI.

17
00:00:51,560 --> 00:00:56,720
Instead of measuring the heights of every single student at UAI,

18
00:00:56,720 --> 00:00:58,400
we take a sample--

19
00:00:58,400 --> 00:01:02,650
for example, 100 randomly selected students.

20
00:01:02,650 --> 00:01:07,730
This gives us a data set of 100 height values.

21
00:01:07,730 --> 00:01:11,090
Now we can begin to visualize this data,

22
00:01:11,090 --> 00:01:15,370
for instance, by plotting the heights of all sampled students

23
00:01:15,370 --> 00:01:17,690
side by side.

24
00:01:17,690 --> 00:01:20,770
Of course, we can sort values to identify

25
00:01:20,770 --> 00:01:26,050
trends, which is that most students are similar in height.

26
00:01:26,050 --> 00:01:29,810
So what's the typical student's height?

27
00:01:29,810 --> 00:01:33,650
There are three common ways to describe that.

28
00:01:33,650 --> 00:01:38,350
We can calculate the arithmetic average height, the mean,

29
00:01:38,350 --> 00:01:41,290
which is 65 inches.

30
00:01:41,290 --> 00:01:43,770
Next, we can find the median, which

31
00:01:43,770 --> 00:01:47,890
represents the middle value when data is sorted.

32
00:01:47,890 --> 00:01:52,530
That is, 50 students are shorter and 50 students

33
00:01:52,530 --> 00:01:55,530
are taller than this value.

34
00:01:55,530 --> 00:01:58,890
And finally, we can also identify the mode, which

35
00:01:58,890 --> 00:02:02,410
is the most frequent value.

36
00:02:02,410 --> 00:02:07,070
We can group these heights into bins of equal size and count

37
00:02:07,070 --> 00:02:10,389
how many students fall into each bin.

38
00:02:10,389 --> 00:02:12,750
This process of counting and binning

39
00:02:12,750 --> 00:02:15,870
is the foundation of a visualization technique

40
00:02:15,870 --> 00:02:18,990
used to explore distributions.

41
00:02:18,990 --> 00:02:22,790
Here we can see where most students' heights are centered

42
00:02:22,790 --> 00:02:26,670
and how they are spread across the range.

43
00:02:26,670 --> 00:02:29,210
A few individuals are relatively tall,

44
00:02:29,210 --> 00:02:33,390
but most heights cluster around the center.

45
00:02:33,390 --> 00:02:36,370
Now, instead of using individual dots,

46
00:02:36,370 --> 00:02:41,670
we can use a bar chart to create a histogram.

47
00:02:41,670 --> 00:02:46,390
A histogram shows how often values occur in each bin,

48
00:02:46,390 --> 00:02:49,910
or in our case, a height range.

49
00:02:49,910 --> 00:02:54,910
This gives us a clear picture of the distribution of our sample,

50
00:02:54,910 --> 00:03:00,190
where most students are and how the rest are spread out.

51
00:03:00,190 --> 00:03:03,230
We can change the bin size--

52
00:03:03,230 --> 00:03:06,040
the integral size-- to see how much

53
00:03:06,040 --> 00:03:09,120
variability is visible in the data.

54
00:03:09,120 --> 00:03:11,600
A smaller bin size shows variations

55
00:03:11,600 --> 00:03:15,880
at a higher granularity, while larger bin sizes

56
00:03:15,880 --> 00:03:18,200
reduce visible detail.

57
00:03:18,200 --> 00:03:22,120
So if the bins are too small, the result

58
00:03:22,120 --> 00:03:26,020
can be noisy, whereas if they are too large,

59
00:03:26,020 --> 00:03:30,080
the distribution can become oversimplified.

60
00:03:30,080 --> 00:03:34,920
We can also present the data using Kernel Density Estimation,

61
00:03:34,920 --> 00:03:39,080
or KDE, which provides a smooth alternative

62
00:03:39,080 --> 00:03:41,760
to the traditional histogram.

63
00:03:41,760 --> 00:03:46,920
Unlike histograms, KDE does not rely on bin sizes

64
00:03:46,920 --> 00:03:50,720
and offers a clearer visualization of the shape

65
00:03:50,720 --> 00:03:53,240
of the data distribution.

66
00:03:53,240 --> 00:03:59,440
Another alternative is a violin plot, which is a mirrored KDE.

67
00:03:59,440 --> 00:04:03,120
It is useful for comparing two distributions,

68
00:04:03,120 --> 00:04:06,260
like the distribution of the heights for male and female

69
00:04:06,260 --> 00:04:09,220
students in our example.

70
00:04:09,220 --> 00:04:13,780
Speaking of data shape, skewness is a useful measure

71
00:04:13,780 --> 00:04:19,899
that captures the asymmetry, or tilt, of a data distribution.

72
00:04:19,899 --> 00:04:23,940
It helps us understand whether the data are biased or at one

73
00:04:23,940 --> 00:04:25,980
side of the distribution.

74
00:04:25,980 --> 00:04:29,020
For example, if the points are evenly

75
00:04:29,020 --> 00:04:31,380
distributed around the center, the data

76
00:04:31,380 --> 00:04:34,300
are symmetrical, showing no skew.

77
00:04:34,300 --> 00:04:37,740
If most data points are clustered toward the left,

78
00:04:37,740 --> 00:04:43,020
with a long tail on the right, we call that a positive skew.

79
00:04:43,020 --> 00:04:45,980
If the most points are clustered toward the right,

80
00:04:45,980 --> 00:04:50,260
with a long tail on the left, that's a negative skew.

81
00:04:50,260 --> 00:04:52,980
In a perfectly symmetrical distribution,

82
00:04:52,980 --> 00:04:57,300
the mean, median, and mode are identical.

83
00:04:57,300 --> 00:05:01,300
However, in asymmetrical or skewed distributions,

84
00:05:01,300 --> 00:05:04,500
these three measures differ.

85
00:05:04,500 --> 00:05:07,590
Let's talk about outliers.

86
00:05:07,590 --> 00:05:13,710
An outlier is an unusually high or low value in our data set.

87
00:05:13,710 --> 00:05:17,050
While, in many cases, outliers are expected,

88
00:05:17,050 --> 00:05:19,670
they can also occur due to an error.

89
00:05:19,670 --> 00:05:22,830
For example, let's say we are conducting

90
00:05:22,830 --> 00:05:26,190
our study on students' heights by asking students

91
00:05:26,190 --> 00:05:29,390
to report their heights, and one of the students

92
00:05:29,390 --> 00:05:34,630
jokingly reports their height as the height of the Eiffel Tower.

93
00:05:34,630 --> 00:05:37,230
Now, when we plot the results, the histogram

94
00:05:37,230 --> 00:05:39,590
might look something like this.

95
00:05:39,590 --> 00:05:41,650
Our mean is now distorted.

96
00:05:41,650 --> 00:05:45,470
It rises to over 190 inches.

97
00:05:45,470 --> 00:05:51,550
This shows how a single outlier can heavily influence the mean.

98
00:05:51,550 --> 00:05:56,230
However, the median remains at about 65 inches,

99
00:05:56,230 --> 00:06:00,630
reflecting the true central tendency of the data.

100
00:06:00,630 --> 00:06:05,030
When we inspect the data set, we see that this extremely high

101
00:06:05,030 --> 00:06:08,290
value is clearly erroneous.

102
00:06:08,290 --> 00:06:12,730
We can safely remove it in order to effectively analyze our data.

103
00:06:12,730 --> 00:06:16,330
This is why it is always important to visually inspect

104
00:06:16,330 --> 00:06:21,250
your data before conducting an analysis or drawing conclusions.

105
00:06:21,250 --> 00:06:24,290
Make sure everything makes sense.

106
00:06:24,290 --> 00:06:27,850
So going back to our descriptive statistics,

107
00:06:27,850 --> 00:06:32,370
the mean is appropriate to use when the data is symmetrical.

108
00:06:32,370 --> 00:06:34,650
The median is particularly useful

109
00:06:34,650 --> 00:06:37,530
when outliers are present, since it

110
00:06:37,530 --> 00:06:40,570
is more robust to extreme values,

111
00:06:40,570 --> 00:06:42,450
like the one we just saw.

112
00:06:42,450 --> 00:06:44,610
Finally, the mode is typically used

113
00:06:44,610 --> 00:06:48,690
in categorical data, where it represents the most frequently

114
00:06:48,690 --> 00:06:51,450
occurring category.

115
00:06:51,450 --> 00:06:55,370
So let's talk about variance and standard deviation.

116
00:06:55,370 --> 00:06:59,850
Variance and standard deviation are the two most common ways

117
00:06:59,850 --> 00:07:03,690
to describe how spread-out values are in a continuous data

118
00:07:03,690 --> 00:07:05,250
set.

119
00:07:05,250 --> 00:07:09,340
Sample variance is the sum of the squared deviations

120
00:07:09,340 --> 00:07:13,620
from the sample mean divided by n minus 1,

121
00:07:13,620 --> 00:07:15,900
where n is the sample size.

122
00:07:15,900 --> 00:07:20,540
The standard deviation is the square root of this variance.

123
00:07:20,540 --> 00:07:24,460
Both of these quantify how much individual values

124
00:07:24,460 --> 00:07:27,340
deviate from the mean.

125
00:07:27,340 --> 00:07:30,300
Why is standard deviation important?

126
00:07:30,300 --> 00:07:34,060
Standard deviation shows how spread-out data points

127
00:07:34,060 --> 00:07:36,260
are around the mean.

128
00:07:36,260 --> 00:07:40,620
It helps us understand the consistency of the data.

129
00:07:40,620 --> 00:07:44,340
Smaller values mean data are tightly clustered,

130
00:07:44,340 --> 00:07:48,300
while larger values indicate greater spread and more

131
00:07:48,300 --> 00:07:50,220
variation.

132
00:07:50,220 --> 00:07:56,300
It shows the average distance of the data point from the mean.

133
00:07:56,300 --> 00:07:59,380
In a normal distribution, something particularly

134
00:07:59,380 --> 00:08:01,540
interesting happens.

135
00:08:01,540 --> 00:08:07,380
About 68% of the data falls within one standard deviation

136
00:08:07,380 --> 00:08:09,200
of the mean--

137
00:08:09,200 --> 00:08:13,840
that is, between the mean minus 1 standard deviation

138
00:08:13,840 --> 00:08:18,240
and the mean plus 1 standard deviation.

139
00:08:18,240 --> 00:08:23,200
Furthermore, if we extend this to the mean minus two

140
00:08:23,200 --> 00:08:28,360
standard deviations and mean plus two standard deviations,

141
00:08:28,360 --> 00:08:33,360
we will include 95% of the sample.

142
00:08:33,360 --> 00:08:38,200
Lastly, when we look at the range from the mean minus three

143
00:08:38,200 --> 00:08:43,400
standard deviations to the mean plus three standard deviations,

144
00:08:43,400 --> 00:08:45,640
the range will include almost all

145
00:08:45,640 --> 00:08:51,320
of the sample, or approximately 99.7%.

146
00:08:51,320 --> 00:08:59,640
This is known as the empirical rule, or the 68-95-99.7 rule,

147
00:08:59,640 --> 00:09:05,200
and it applies only to normal distributions.

148
00:09:05,200 --> 00:09:09,650
Let's also talk about quantiles and percentiles.

149
00:09:09,650 --> 00:09:13,690
Quantiles are values that divide a data set

150
00:09:13,690 --> 00:09:17,090
into equal-sized intervals.

151
00:09:17,090 --> 00:09:22,930
For example, quartiles split the data into four equal parts

152
00:09:22,930 --> 00:09:27,490
so that each part contains approximately the same number

153
00:09:27,490 --> 00:09:29,530
of data points.

154
00:09:29,530 --> 00:09:34,130
Percentiles, on the other hand, refer to the position

155
00:09:34,130 --> 00:09:37,410
of values within a data set.

156
00:09:37,410 --> 00:09:41,210
For example, if we have 100 students,

157
00:09:41,210 --> 00:09:44,410
and we divide them into quartiles,

158
00:09:44,410 --> 00:09:49,130
there will be 25 students in each quartile.

159
00:09:49,130 --> 00:09:54,810
We can then refer to these data points using percentiles,

160
00:09:54,810 --> 00:09:57,090
which represent the rank position

161
00:09:57,090 --> 00:10:01,010
of a value within the data set.

162
00:10:01,010 --> 00:10:04,450
The boxplot is an important visualization tool

163
00:10:04,450 --> 00:10:08,990
because it summarizes five key statistics.

164
00:10:08,990 --> 00:10:12,510
Here we can see the first quartile,

165
00:10:12,510 --> 00:10:16,550
which corresponds to the 25th percentile,

166
00:10:16,550 --> 00:10:23,790
meaning that 25% of the data values fall below this point.

167
00:10:23,790 --> 00:10:27,230
The median, which is the second quartile,

168
00:10:27,230 --> 00:10:32,230
or the 50th percentile, indicates that half the data

169
00:10:32,230 --> 00:10:37,470
falls below that value and half above.

170
00:10:37,470 --> 00:10:42,350
The third quartile, which is the 75th percentile,

171
00:10:42,350 --> 00:10:48,470
meaning that 75% of the data lies below this value and 25%

172
00:10:48,470 --> 00:10:51,190
above.

173
00:10:51,190 --> 00:10:54,790
And as the fourth and fifth key statistic,

174
00:10:54,790 --> 00:10:59,350
we can also see the minimum and the maximum that do not

175
00:10:59,350 --> 00:11:03,190
include outliers on the plot.

176
00:11:03,190 --> 00:11:07,330
So it also helps us visualize the center, spread,

177
00:11:07,330 --> 00:11:11,500
and potential outliers as dots in our data.

